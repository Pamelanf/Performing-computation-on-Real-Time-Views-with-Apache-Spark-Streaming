{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK_STREAMING_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/hadoop'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "import time\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading streaming data\n",
    "\n",
    "def remove_quotation(x):\n",
    "        return ([xx.replace('\"','') for xx in x])\n",
    "\n",
    "def load():\n",
    "    \n",
    "    downloadsRDD = ssc.textFileStream(\"file:///usr/local/hadoop/streaming/\")\n",
    "    downloadsRDD = downloadsRDD.map(lambda x: x.split(','))\n",
    "    downloadsRDD = downloadsRDD.map(remove_quotation)\n",
    "    return downloadsRDD  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting, Computing, and Saving Streaming Computations \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q1. To calculate the number of downloads of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To store the Results of Streaming computations in Cassandra\n",
    "def save_1(time, rdd):\n",
    "    try:\n",
    "        df = spark.createDataFrame(rdd.map(lambda row: Row(time=time, package=row[0], count=row[1])))\n",
    "        df.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"q1\", keyspace=\"streaming\").save(mode=\"append\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#Streaming Computation with batch lengh of 5 seconds\n",
    "ssc = StreamingContext (sc,5)\n",
    "\n",
    "#Calling function load() to load streaming data\n",
    "lines=load()\n",
    "\n",
    "#Streaming computation\n",
    "down_each_pack = lines.map(lambda x: (x[6],1))\n",
    "down_each_pack = down_each_pack.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "#Saving in Apache Cassandra\n",
    "down_each_pack.foreachRDD(save_1)\n",
    "\n",
    "# Start Streaming\n",
    "ssc.start()\n",
    "time.sleep(30)\n",
    "ssc.stop(stopSparkContext=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.To find the top most downloaded package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the Results of Streaming computations in Cassandra\n",
    "\n",
    "def save_2(time, rdd):\n",
    "    try:\n",
    "        df = spark.createDataFrame(rdd.map(lambda row: Row(time=time, top_most_down_package=row[0], count=row[1])))\n",
    "        df.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"q2\", keyspace=\"streaming\").save(mode=\"append\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#Streaming Computation with batch lengh of 5 seconds\n",
    "ssc = StreamingContext (sc,5)\n",
    "\n",
    "#Calling function load() to load streaming data\n",
    "lines=load()\n",
    "    \n",
    "#Streaming computation\n",
    "top_1 = lines.map(lambda x:(x[6],1))\n",
    "top_1 = top_1.reduceByKey(lambda a,b: a+b)\n",
    "top_1_down_pack = top_1.transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))\n",
    "top_1_down_pack = top_1_down_pack.transform(lambda rdd:sc.parallelize(rdd.take(1)))\n",
    "\n",
    "#Saving in Apache Cassandr\n",
    "top_1_down_pack.foreachRDD(save_2)\n",
    "\n",
    "# Start Streaming\n",
    "ssc.start()\n",
    "time.sleep(50)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.To find the top 5 countries along with number of downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: To store the Results of Streaming computations in Cassandra\n",
    "def save_3(time, rdd):\n",
    "    try:\n",
    "        df = spark.createDataFrame(rdd.map(lambda row: Row(time=time, top_5_countries=row[0], count=row[1])))\n",
    "        df.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"q3\", keyspace=\"streaming\").save(mode=\"append\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Step 2: Streaming Computation with batch lengh of 5 seconds\n",
    "ssc = StreamingContext (sc,5)\n",
    "\n",
    "# Step 3: Calling function load() to load streaming data\n",
    "lines=load()\n",
    "    \n",
    "# Step 4: Streaming computation\n",
    "top_5 = lines.map(lambda x: (x[8],1))\n",
    "top_5 = top_5.reduceByKey(lambda a,b: a+b)\n",
    "top_5_countries = top_5.transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False))\n",
    "top_5_countries = top_5_countries.transform(lambda rdd:sc.parallelize(rdd.take(5)))\n",
    "\n",
    "# Step 5: Saving in Apache Cassandra\n",
    "top_5_countries.foreachRDD(save_3)\n",
    "\n",
    "# Step 6: Start Streaming\n",
    "ssc.start()\n",
    "time.sleep(50)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. To find number of downloads for ggplot2 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: To store the Results of Streaming computations in Cassandra\n",
    "def save_4(time, rdd):\n",
    "    try:\n",
    "        df = spark.createDataFrame(rdd.map(lambda row: Row(time=time, package=row[0], count=row[1])))\n",
    "        df.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"q4\", keyspace=\"streaming\").save(mode=\"append\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Step 2: Streaming Computation with batch lengh of 5 seconds\n",
    "ssc = StreamingContext (sc,5)\n",
    "\n",
    "# Step 3: Calling function load() to load streaming data\n",
    "\n",
    "lines=load()\n",
    "\n",
    "#Step 4: Streaming computation\n",
    "ggplot2 = lines.map(lambda x: (x[6],1))\n",
    "ggplot2 = ggplot2.reduceByKey(lambda a,b: a+b)\n",
    "ggplot2_download = ggplot2.filter(lambda x: x[0]=='ggplot2')\n",
    "\n",
    "#Step 5: Saving in Apache Cassandra\n",
    "ggplot2_download.foreachRDD(save_4)\n",
    "\n",
    "# Step 6: Start Streaming\n",
    "ssc.start()\n",
    "time.sleep(50)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
